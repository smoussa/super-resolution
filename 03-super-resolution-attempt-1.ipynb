{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "matplotlib.rc('figure', figsize=(12, 5))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image loading and processing\n",
    "\n",
    "Common pretrained vision models require us to normalise images with a mean and std vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_vec = torch.FloatTensor([ 0.485, 0.456, 0.406 ]).view(3,1,1)\n",
    "std_vec = torch.FloatTensor([ 0.229, 0.224, 0.225 ]).view(3,1,1)\n",
    "\n",
    "# convert tensor to image for viewing\n",
    "tensor_to_image = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x * std_vec + mean_vec),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "# create low resolution square images\n",
    "def input_transform(crop_size, upscale_factor):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Scale(crop_size // upscale_factor), # downsample\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_vec, std_vec)\n",
    "    ])\n",
    "\n",
    "# maintain high resolution square images\n",
    "def target_transform(crop_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_vec, std_vec)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load each image as a duplicate pair\n",
    "class DoubleImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, input_transform=None, target_transform=None):\n",
    "        super(DoubleImageDataset, self).__init__()\n",
    "        \n",
    "        self.image_fnames = [os.path.join(img_dir, f) for f in os.listdir(img_dir)]\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_img = Image.open(self.image_fnames[index]).convert('RGB')\n",
    "        target_img = input_img.copy()\n",
    "        \n",
    "        if self.input_transform:\n",
    "            input_img = self.input_transform(input_img)\n",
    "        if self.target_transform:\n",
    "            target_img = self.target_transform(target_img)\n",
    "            \n",
    "        return input_img, target_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = '/home/samir/Downloads/ILSVRC2012_img_train/train'\n",
    "upscale_factor = 2\n",
    "crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "train_dataset = DoubleImageDataset(\n",
    "    dirname,\n",
    "    input_transform=input_transform(crop_size, upscale_factor),\n",
    "    target_transform=target_transform(crop_size))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8, # INCREASE BATCH SIZE\n",
    "    shuffle=True,\n",
    "    num_workers=7,\n",
    "    pin_memory=True)\n",
    "\n",
    "# preview some images\n",
    "low_res, high_res = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_to_image(low_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_to_image(high_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained model to visualise activations\n",
    "\n",
    "- Load pretrained model\n",
    "- Pass through some test images to inspect activations at certain layers\n",
    "\n",
    "(VGG specific) We want to see what each of the convolutional layers will output (after ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model = models.vgg16_bn(pretrained=True).features\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children = list(pretrained_model.children())\n",
    "layer_idxs = [0, 2, 5, 9, 12, 16] #  just before next Conv2d\n",
    "layer_weights = [0.05, 0.05, 0.1, 0.1, 0.35, 0.35]\n",
    "conv_models = [(copy.deepcopy(nn.Sequential(*children[:i+1])), w)\n",
    "               for i, w in zip(layer_idxs, layer_weights)]\n",
    "\n",
    "assert conv_models[0][0] is not conv_models[1][0]\n",
    "conv_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img = high_res[0].unsqueeze(0)\n",
    "\n",
    "out_imgs = []\n",
    "for conv_model, _ in conv_models:\n",
    "    conv_model = conv_model.cuda()\n",
    "    out_img = conv_model(Variable(test_img).cuda())\n",
    "    out_img = out_img.cpu().squeeze().data\n",
    "    out_imgs.append(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_idx = 30\n",
    "gs = matplotlib.gridspec.GridSpec(2, 3)\n",
    "\n",
    "for i, g in enumerate(gs):\n",
    "    ax = plt.subplot(g)\n",
    "    ax.imshow(out_imgs[i][filter_idx])\n",
    "    ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling model \n",
    "\n",
    "- Takes in a low resolution image and upscales it to the same size as its target\n",
    "- This network attemps to match its target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(n_in, n_out, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_out)\n",
    "        self.conv2 = nn.Conv2d(n_out, n_out, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(n_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply residual block\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        \n",
    "        # WHY USE A CONV AFTER UPSAMPLING?\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv = nn.Conv2d(64, 64, 3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "\n",
    "class UpsamplingNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(UpsamplingNetwork, self).__init__()\n",
    "        \n",
    "        self.receptor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 9, padding=4, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.resblock1 = ResidualBlock(64, 64)\n",
    "        self.resblock2 = ResidualBlock(64, 64)\n",
    "        self.resblock3 = ResidualBlock(64, 64)\n",
    "        self.resblock4 = ResidualBlock(64, 64)\n",
    "        \n",
    "        self.upsampler1 = UpsampleBlock() # TRY ONLY ONE UPSAMPLE BLOCK\n",
    "#         self.upsampler2 = UpsampleBlock()\n",
    "        \n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 9, padding=4, bias=False),\n",
    "            nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.receptor(x)\n",
    "        \n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        \n",
    "        x = self.upsampler1(x)\n",
    "#         x = self.upsampler2(x)\n",
    "        \n",
    "        x = self.reducer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upsampling_net = UpsamplingNetwork().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual loss model\n",
    "\n",
    "- Construct perceptual loss model from required pretrained model layers\n",
    "- When forward passing through the perceptual loss model, forward pass through every subset conv model.\n",
    "- Test outputs\n",
    "\n",
    "*Need to clamp input?*\n",
    "*Need to make input to percept loss volatile.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PerceptualLossNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation_models):\n",
    "        super(PerceptualLossNetwork, self).__init__()\n",
    "        \n",
    "        for model, _ in activation_models:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.activation_models = activation_models\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for model, weight in self.activation_models:\n",
    "            x_activations = model(x)\n",
    "            target_activations = model(target)\n",
    "            total_loss += self.loss_fn(x_activations, target_activations) * weight\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percept_net = PerceptualLossNetwork(conv_models).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimiser = optim.Adam(upsampling_net.parameters(), lr=1e-4) # CHANGE OPTIMISER\n",
    "# optimiser = optim.LBFGS(upsampling_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "print('Training ...')\n",
    "start_time = time.time()\n",
    "weights = upsampling_net.state_dict()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {:3d}/{:3d}'.format(epoch, num_epochs))\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (input_img, target_img) in enumerate(data_loader, 1):\n",
    "\n",
    "        # load batch\n",
    "        input_img = Variable(input_img, requires_grad=False).cuda(async=True)\n",
    "        target_img = Variable(target_img, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # clear gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        predicted_img = upsampling_net(input_img)\n",
    "\n",
    "        # get loss\n",
    "        percept_loss = percept_net(predicted_img, target_img)\n",
    "        \n",
    "        # compute gradients\n",
    "        percept_loss.backward()\n",
    "\n",
    "        # optimise\n",
    "        optimiser.step()\n",
    "        \n",
    "        print('Batch loss: {:4f}'.format(percept_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.save(upsampling_net.state_dict(), './model-35min.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_time = (time.time() - start_time) / 60\n",
    "end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test upsampling network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_res, high_res = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 4\n",
    "y = upsampling_net(Variable(low_res).cuda())\n",
    "y = y.cpu().data[idx] * 1.5\n",
    "print(y.min(), y.max())\n",
    "tensor_to_image(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
