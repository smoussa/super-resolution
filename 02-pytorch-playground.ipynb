{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Need to read convolution arithmetic guide\n",
    "- Perceptual loss network does not have to be a whole module.\n",
    "- Add learning rate decay\n",
    "- Models and parameters must be moved to the GPU before the optimiser is created.\n",
    "- Use pinned memory\n",
    "\n",
    "**How the optimiser works with a closure**\n",
    "\n",
    "Let's say we've created an optimiser as such:\n",
    "\n",
    "`optimiser = optim.SGD(model.parameters())`\n",
    "\n",
    "When `loss.backward()` is called, the model's Variables are given gradient values so that when we call `optimiser.step()`, the optimiser updates the parameters of the model with their respective gradients.\n",
    "\n",
    "If a closure argument is given to `optimiser.step(closure)`, we assume that the optimiser wishes to keep a history of the losses or some internal state which may involve calling the closure multiple times before the actual optimisation step is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! ls $dirname'/sample' | tail -100\n",
    "# Image.open(dirname+'/sample/'+'n09332890_8608.JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "matplotlib.rc('figure', figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean and std are specific to pretrained models\n",
    "mean_vec = torch.FloatTensor([ 0.485, 0.456, 0.406 ]).view(3,1,1)\n",
    "std_vec = torch.FloatTensor([ 0.229, 0.224, 0.225 ]).view(3,1,1)\n",
    "\n",
    "# preproces PIL image\n",
    "# return tensor of shape (batch, channels, height, width)\n",
    "image_to_tensor = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_vec, std_vec)\n",
    "])\n",
    "\n",
    "tensor_to_image = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x * std_vec + mean_vec),\n",
    "    transforms.ToPILImage()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "def input_transform(crop_size, upscale_factor):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Scale(crop_size // upscale_factor), # downsample\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_vec, std_vec)\n",
    "    ])\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_vec, std_vec)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImagesOnlyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transform=None):\n",
    "        super(ImagesOnlyDataset, self).__init__()\n",
    "        \n",
    "        self.images = [os.path.join(root, f) for f in os.listdir(root)]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DoubleImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, input_transform=None, target_transform=None):\n",
    "        super(DoubleImageDataset, self).__init__()\n",
    "        \n",
    "        self.image_fnames = [os.path.join(img_dir, f) for f in os.listdir(img_dir)]\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_img = Image.open(self.image_fnames[index]).convert('RGB')\n",
    "        target_img = input_img.copy()\n",
    "        \n",
    "        if self.input_transform:\n",
    "            input_img = self.input_transform(input_img)\n",
    "        if self.target_transform:\n",
    "            target_img = self.target_transform(target_img)\n",
    "            \n",
    "        return input_img, target_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = '/home/samir/Downloads/ILSVRC2012_img_train/train'\n",
    "upscale_factor = 2\n",
    "crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "train_dataset = DoubleImageDataset(\n",
    "    dirname,\n",
    "    input_transform=input_transform(crop_size, upscale_factor),\n",
    "    target_transform=target_transform(crop_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4, # INCREASE BATCH SIZE\n",
    "    shuffle=True,\n",
    "    num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res, high_res = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(low_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(high_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct network\n",
    "\n",
    "**Blocks required:**\n",
    "\n",
    "- Semantic network - trainable network used for objective inference\n",
    "- Perceptual loss network - ouputs the loss between the activations of two inputs at some layer\n",
    "- Training loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = models.vgg16_bn(pretrained=True).features\n",
    "pretrained_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model_subset = nn.Sequential(*list(pretrained_model.children())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model_subset = pretrained_model_subset.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(pretrained_model_subset(Variable(low_res).cuda()).cpu().data[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(low_res[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(n_in, n_out, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_out)\n",
    "        self.conv2 = nn.Conv2d(n_out, n_out, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(n_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply residual block\n",
    "        residual = x # COULD BE ERROR IN VARIABLE NAMES\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out) # PROBABLY DONT NEED THIS\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear')\n",
    "        self.conv = nn.Conv2d(64, 64, 3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "\n",
    "class SemanticNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SemanticNetwork, self).__init__()\n",
    "        \n",
    "        self.receptor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 9, padding=4, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.resblock1 = ResidualBlock(64, 64)\n",
    "        self.resblock2 = ResidualBlock(64, 64)\n",
    "        self.resblock3 = ResidualBlock(64, 64)\n",
    "        self.resblock4 = ResidualBlock(64, 64)\n",
    "        \n",
    "        self.upsampler1 = UpsampleBlock() # TRY ONLY ONE UPSAMPLE BLOCK\n",
    "        self.upsampler2 = UpsampleBlock()\n",
    "        \n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 9, padding=4, bias=False),\n",
    "            nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.receptor(x)\n",
    "        \n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        \n",
    "        x = self.upsampler1(x)\n",
    "        x = self.upsampler2(x)\n",
    "        \n",
    "        x = self.reducer(x)\n",
    "        \n",
    "        x = (x + 1) * 127.5\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PerceptualLossNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model, activation_layer):\n",
    "        super(PerceptualLossNetwork, self).__init__()\n",
    "        \n",
    "        layers = list(pretrained_model.children())[:activation_layer]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.loss_criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        # PERCEPTUAL LOSS IS MULTIPLE LAYERS (with weights)\n",
    "        \n",
    "        x_activations = self.net(x)\n",
    "        target_activations = self.net(target)\n",
    "        \n",
    "        loss = self.loss_criterion(x_activations, target_activations)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activation_layer = 4+ 3 # MIGHT NEED A LATER ACTIVATION LAYER\n",
    "\n",
    "semantic_net = SemanticNetwork()\n",
    "\n",
    "percept_net = PerceptualLossNetwork(pretrained_model_subset, activation_layer)\n",
    "for param in percept_net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to GPU\n",
    "semantic_net = semantic_net.cuda()\n",
    "percept_net = percept_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and optimisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optim.Adam(semantic_net.parameters(), lr=1e-3) # CHANGE OPTIMISER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = None\n",
    "y = None\n",
    "z = None\n",
    "\n",
    "def train(num_epochs=5):\n",
    "    \n",
    "    global x\n",
    "    global y\n",
    "    global z\n",
    "    \n",
    "    print('Training ...')\n",
    "    start_time = time.time()\n",
    "    best_weights = semantic_net.state_dict()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {:3d}/{:3d}'.format(epoch, num_epochs))\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, (input_img, target_img) in enumerate(data_loader):\n",
    "            \n",
    "            # load batch\n",
    "            input_img = Variable(input_img).cuda()\n",
    "            target_img = Variable(target_img).cuda()\n",
    "            \n",
    "            print(input_img.shape)\n",
    "            x = input_img\n",
    "            \n",
    "            # clear gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            predicted_img = semantic_net(input_img)\n",
    "            \n",
    "            print(predicted_img.shape)\n",
    "            z = predicted_img\n",
    "            \n",
    "            percept_loss = percept_net(predicted_img, target_img)\n",
    "            \n",
    "            print(target_img.shape)\n",
    "            y = target_img\n",
    "            \n",
    "            # optimise\n",
    "            optimiser.step()\n",
    "            \n",
    "            break\n",
    "            \n",
    "            print('Batch loss {:4f}'.format(percept_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(y.cpu().data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tensor_to_image(percept_net.net(z).cpu().data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
