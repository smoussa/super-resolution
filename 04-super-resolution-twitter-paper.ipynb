{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "matplotlib.rc('figure', figsize=(12, 5))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean_vec = torch.FloatTensor([ 0.485, 0.456, 0.406 ]).view(3,1,1)\n",
    "# std_vec = torch.FloatTensor([ 0.229, 0.224, 0.225 ]).view(3,1,1)\n",
    "\n",
    "# convert tensor to image for viewing\n",
    "tensor_to_image = transforms.Compose([\n",
    "#     transforms.Lambda(lambda x: x * std_vec + mean_vec),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "# create low resolution square images\n",
    "def input_transform(crop_size, upscale_factor):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Scale(crop_size // upscale_factor), # downsample\n",
    "        transforms.ToTensor()\n",
    "#         transforms.Normalize(mean_vec, std_vec)\n",
    "    ])\n",
    "\n",
    "# maintain high resolution square images\n",
    "def target_transform(crop_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "#         transforms.Normalize(mean_vec, std_vec)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load each image as a duplicate pair\n",
    "class DoubleImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, input_transform=None, target_transform=None):\n",
    "        super(DoubleImageDataset, self).__init__()\n",
    "        \n",
    "        self.image_fnames = [os.path.join(img_dir, f) for f in os.listdir(img_dir)]\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input_img = Image.open(self.image_fnames[index]).convert('YCbCr')\n",
    "        input_img, _, _ = input_img.split()\n",
    "        target_img = input_img.copy()\n",
    "        \n",
    "        if self.input_transform:\n",
    "            input_img = self.input_transform(input_img)\n",
    "        if self.target_transform:\n",
    "            target_img = self.target_transform(target_img)\n",
    "            \n",
    "        return input_img, target_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = '/home/samir/Documents/pytorch-examples/super_resolution/dataset/BSDS300/images/train/'\n",
    "upscale_factor = 3\n",
    "crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "train_dataset = DoubleImageDataset(\n",
    "    dirname,\n",
    "    input_transform=input_transform(crop_size, upscale_factor),\n",
    "    target_transform=target_transform(crop_size))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=7,\n",
    "    pin_memory=True)\n",
    "\n",
    "# preview some images\n",
    "low_res, high_res = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_image(low_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor_to_image(high_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UpsamplingNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, upscale_factor):\n",
    "        super(UpsamplingNetwork, self).__init__()\n",
    "        \n",
    "        in_channels = 1\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, (5,5), (1,1), (2,2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3,3), (1,1), (1,1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3,3), (1,1), (1,1))\n",
    "        self.conv4 = nn.Conv2d(32, in_channels*(upscale_factor**2), (3,3), (1,1), (1,1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "        \n",
    "        self._initialise_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "    \n",
    "    def _initialise_weights(self):\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.orthogonal(self.conv1.weight, gain)\n",
    "        init.orthogonal(self.conv2.weight, gain)\n",
    "        init.orthogonal(self.conv3.weight, gain)\n",
    "        init.orthogonal(self.conv4.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upsampling_net = UpsamplingNetwork(upscale_factor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimiser = optim.Adam(upsampling_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "print('Training ...')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {:3d}/{:3d}'.format(epoch, num_epochs))\n",
    "\n",
    "    for i, (input_img, target_img) in enumerate(data_loader, 1):\n",
    "\n",
    "        # load batch\n",
    "        input_img = Variable(input_img).cuda(async=True)\n",
    "        target_img = Variable(target_img).cuda(async=True)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predicted_img = upsampling_net(input_img)\n",
    "        loss = loss_fn(predicted_img, target_img)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        print('Batch loss: {:4f}'.format(loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
